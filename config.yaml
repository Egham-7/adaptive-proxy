---
# Adaptive Backend Configuration
# Environment variables can be referenced using ${VAR} or ${VAR:-default} syntax (defaults apply when a variable is unset or empty)

server:
  port: "${PORT:-8080}"
  allowed_origins: "${ALLOWED_ORIGINS:-http://localhost:3000}"
  environment: "${ENV:-development}"
  log_level: "${LOG_LEVEL:-info}"

# Endpoint-specific provider configurations
endpoints:
  chat_completions:
    providers:
      openai:
        api_key: "${OPENAI_API_KEY}"
        enabled: true

      anthropic:
        api_key: "${ANTHROPIC_API_KEY}"
        enabled: true
        base_url: "https://api.anthropic.com/v1"

      deepseek:
        api_key: "${DEEPSEEK_API_KEY}"
        enabled: true
        base_url: "https://api.deepseek.com"

      gemini:
        api_key: "${GEMINI_API_KEY}"
        enabled: true
        base_url: "https://generativelanguage.googleapis.com/v1beta/openai"

  messages:
    providers:
      anthropic:
        api_key: "${ANTHROPIC_API_KEY}"
        enabled: true
        base_url: "https://api.anthropic.com"

  select_model:
    providers:
      openai:
        api_key: "${OPENAI_API_KEY}"
        enabled: true

      anthropic:
        api_key: "${ANTHROPIC_API_KEY}"
        enabled: true
        base_url: "https://api.anthropic.com/v1"

      deepseek:
        api_key: "${DEEPSEEK_API_KEY}"
        enabled: true
        base_url: "https://api.deepseek.com"

      huggingface:
        api_key: "${HUGGINGFACE_API_KEY}"
        enabled: true
        base_url: "https://router.huggingface.co/v1"

  generate:
    providers:
      gemini:
        api_key: "${GEMINI_API_KEY}"
        enabled: true
        base_url: "https://generativelanguage.googleapis.com/v1beta"

  count_tokens:
    providers:
      gemini:
        api_key: "${GEMINI_API_KEY}"
        enabled: true
        base_url: "https://generativelanguage.googleapis.com/v1beta"

# Model router configuration
model_router:
  cost_bias: 0.9 # 0.0 = cheapest, 1.0 = best performance
  semantic_cache:
    enabled: true
    backend: "${CACHE_BACKEND:-redis}" # "redis" or "memory"
    redis_url: "${REDIS_URL:-redis://localhost:6379}" # Required if backend is "redis"
    capacity: 1000 # Required if backend is "memory" (LRU cache size)
    semantic_threshold: 0.95
    openai_api_key: "${OPENAI_API_KEY}" # For embeddings
  client:
    adaptive_router_url: "${ADAPTIVE_ROUTER_URL:-http://localhost:8000}"
    jwt_secret: "${ADAPTIVE_ROUTER_JWT_SECRET:-dev-secret}"
    timeout_ms: 3000
    circuit_breaker:
      failure_threshold: 3
      success_threshold: 2
      timeout_ms: 5000
      reset_after_ms: 30000

# Fallback configuration
fallback:
  mode: "race" # "race" or "sequential"
  timeout_ms: 30000 # Keep longer for streaming LLM responses
  max_retries: 3
  circuit_breaker:
    failure_threshold: 5
    success_threshold: 3
    timeout_ms: 15000
    reset_after_ms: 60000

# Prompt cache configuration (provider-specific prompt caching)
prompt_cache:
  enabled: false
  backend: "${CACHE_BACKEND:-redis}" # "redis" or "memory"
  redis_url: "${REDIS_URL:-redis://localhost:6379}" # Required if backend is "redis"
  capacity: 1000 # Required if backend is "memory" (LRU cache size)
